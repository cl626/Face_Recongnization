{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 240, 240])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 3, 240, 240])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 3, 240, 240])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 3, 240, 240])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 3, 240, 240])\n",
      "torch.Size([1, 512])\n",
      "torch.Size([1, 3, 240, 240])\n",
      "torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "from facenet_pytorch import MTCNN,InceptionResnetV1\n",
    "import torch\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mtcnn=MTCNN(image_size=240,margin=0,min_face_size=20)\n",
    "resnet=InceptionResnetV1(pretrained='vggface2').eval()\n",
    "\n",
    "dataset=datasets.ImageFolder('./data/test_images')\n",
    "idx_to_class={i:c for c,i in dataset.class_to_idx.items()}\n",
    "\n",
    "def collate_fn(x):\n",
    "    return x[0]\n",
    "\n",
    "loader=DataLoader(dataset,collate_fn=collate_fn)\n",
    "\n",
    "face_list=[]\n",
    "name_list=[]\n",
    "embedding_list=[]\n",
    "for img,idx in loader:\n",
    "    face,prob =mtcnn(img,return_prob=True)\n",
    "    print((face.unsqueeze(0)).shape)\n",
    "    if face is not None and prob>0.90:\n",
    "        emb=resnet(face.unsqueeze(0))\n",
    "        print(emb.shape)\n",
    "        embedding_list.append(emb.detach())\n",
    "        name_list.append(idx_to_class[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* What's role of embedding_list&name_list?\n",
    "\n",
    "Formation of image's embedding & name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[embedding_list,name_list]\n",
    "torch.save(data,'data.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "malong\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "def face_match(img_path,data_path):\n",
    "    img=Image.open(img_path)\n",
    "    face,prob = mtcnn(img,return_prob=True)\n",
    "    emb=resnet(face.unsqueeze(0)).detach()\n",
    "    \n",
    "    saved_data = torch.load('data.pt')\n",
    "    embedding_list = saved_data[0]\n",
    "    name_list = saved_data[1]\n",
    "    dist_list = []\n",
    "\n",
    "    for idx,emb_db in enumerate(embedding_list):\n",
    "        dist = torch.dist(emb,emb_db).item()\n",
    "        dist_list.append(dist)\n",
    "\n",
    "    idx_min = dist_list.index(min(dist_list))\n",
    "\n",
    "    return name_list[idx_min]\n",
    "\n",
    "person=face_match('ml3.jpg','data.pt')\n",
    "print(person)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
